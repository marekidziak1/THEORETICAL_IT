5. ZAPIS DANYCH DO PLIKÓW:
	1)CSV:
			**odczyt:
					df = pd.read_csv('survey.csv')
			**zapis:
					df.to_csv('data/india_survey.csv')
		*TSV - TAB CSV (jako separator używasz tabulatora:
			**odczyt:
					df = pd.read_csv('survey.tsv', sep='\t')
			**zapis:
					df.to_csv('survey.tsv', sep='\t')
	2)EXCEL:
		*potrzebne bilioteki do pracy na plikach excel:
					pip install wheel			#potrzebne do instalacji poniższych
					pip install xlwt			#zapis do .XLS (stary format)
					pip install openpyxl		#zapis do .XLSX (nowy format)
					pip install xlrd			#pip install xlrd
			**odczyt:
					pd.read_excel('data/survey.xlsx', index_col='Respondent')
			**zapis:
					df.to_excel('survey.xlsx')
	3)JSON:
			**odczyt:
					pd.read_json('data/india_survey.json')
			**zapis w SCHEMACIE DOMYŚLNYM - słownik gdzie kolumny to klucze a 
			  wartości to słownik z kluczami będacymi indexami i wartościami w 
			  postaci komórek:
					india_df.to_json('survey.json')
			**zapis w SCHEMACIE gdzie każdy wiersz/rekord to słownik z kluczami 
			  będącymi kolumnami i wartościami w postaci komórek (taki schemat nie 
			  zapisuje INDEXU!!!):
					df.to_json('survey_2.json', orient='records', lines=True)
	4)SQL:
		*do połączenia się z bazą danych SQLAlchemy - ORM dla Pythona (w przypadku 
		 POSTGRESA trzeba jeszcze dodatkowo zainstalować psicopg2-binary):
					pip install SQLAlchemy
					pip install psicopg2-binary
		*najpierw trzeba utworzyć połączenie z bazą danych SQL (zamiast dbuser i 
		 dbpass powinno się mieć jakiś plik config ze zmiennymi)
					from sqlalchemy import create_engine
					import psycopg2
					engine = create_engine('postgresql://dbuser:dbpass:5432/my_db')
			**odczyt tabeli:
					df = pd_read_sql('sample_table', engine, index_col='Respondent')
			**odczyt wyekstraktowanej tabeli wg zapytania SQL:
					df = pd_read_sql_query('SELECT * FROM sample_table', engine, index_col='Respondent')
			**zapis:
					df.to_sql('sample_table', engine, if_exists='replace')
	5)CZYTANIE JAKIEGOKOLWIEK FORMATU Z LINKU URL:
					df_from_url = pd.read_json('https://link/url')
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
















https://www.geeksforgeeks.org/python-pandas-series-combine/			
		https://www.geeksforgeeks.org/python-pandas-tseries-offsets-dateoffset/
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
4. PRACA NA DATACH:
	1)PODSTAWY:
		*daty domyślnie ładują się jako Stringi:
					df = pd.read_csv('data\ETH_1h.csv')
					df.loc[0,'Date']							#'2020-03-13 08-PM'
		*zmiana danych z datą ze Stringów na typ Date (Timestamp):
			**trzeba podać formatowanie (tak jak w pythonie - ze strony: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior
					df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d %I-%p')
			   #lub	df['Date'] = df['Date'].astype('datetime64[ns]')
					df.loc[0,'Date']				#Timestamp('2020-03-13 20:00:00')
			*wyświetlenie  dnia tygodnia:
					df.loc[0,'Date'].day_name()					#'Friday'
		*ładowanie danych wraz ze zmianą typu kolumny na datę:
			**stara opcja - pd.datetime.strptime(...):
					df = pd.read_csv('data\ETH_1h.csv', parse_dates=['Date'], date_parser = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %I-%p'))
			**nowa opcja - datetime.strptime(...):
					from datetime import datetime
					df = pd.read_csv('data\ETH_1h.csv', parse_dates=['Date'], date_parser = lambda x: datetime.strptime(x, '%Y-%m-%d %I-%p'))
		*różnice czasowe (odejmowanie dat (taka timedelta)):
					df['Date'].max() - df['Date'].min() 		#Timedelta('986 days 
																#09:00:00')
		*FILTROWANIE PO DATACH
			**zmiana indexu na kolumnę z datą -> można filtrować te daty jak index 
			  domyślny czyli po slicingu (od do), przy czym nie trzeba wpisywać np 
			  całej daty a wystarczy jej początek (slicing z przedziałami 
			  obustronnie zamk.):
					df.set_index('Date', inplace = True)
					df['2020-01':'2020-02']	#wiersze z datami od Stycznia do Lutego 
					df['2020':'2020']		#wiersze tylko z roku 2020
			**można użyć tez loc[]/iloc[] (przy zmianie indexu na kolumnę z datą)
			  (przedziały TEŻ są obustronnie zamk.):
					df.loc['2020-01':'2020-01']
					df.loc['2020':'2020']
			**w przypadku gdyby data nie była indexem to zakres dat uzyskujesz tak 
			  jak przy zwykłym filtrowaniu (z tym że nie musisz wpisywać pełnego 
			  formatu - wystarczy początek):
					#filt = (df['Date'] == '2019')		#ŹLE: zwróci tylko 1 komórkę
														#zamiast wszystkich z 2019
					filt = (df['Date'] >= '2019') & (df['Date'] < '2020')	
					filt = (df['Date']>='2020-01') & (df['Date']<='2020-02')
					df.loc[filt]
	2)RESAMPLING: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects
		*działa tylko na danych których indexem jest data
					df.set_index('Date', inplace = True)
		*resample tak jakby grupuję wiersze po wskazanym okresie czasowym (okresy 
		 wskazane w linku powyżej) i na każdej grupie wierszy na wybranej kolumnej 
		 jest wywoływana funkcja agregacyjna. [np: mimo że każdy wiersz ma index co 
		 1 minutę to resample grupuję to wg dni i dla kolumny 'High' dla każdego 
		 dnia wyszuka maxymalną wartość (ze wszystkich wierszy odpowiadającym 
		 minutom)]:
					highsOfTheEachDay = df['High'].resample('D').max()
					highsOfTheEachDay						#Date
															#2017-07-01    279.99
															#2017-07-02    293.73
															#2017-07-03    285.00
															#2017-07-04    282.83
		*można wywołać też to dla wszystkich kolumn (całej tabeli):
					df.resample('W').mean()
		*można wywołać też dla wielu kolumn różne funkcje agregujące:
					df.resample('W').agg({'Close':'mean', 'High':'max', 
													'Low':'min', 'Volume':'sum'})
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
3.GROUPBY:
  0.SKRÓT GROUPBY:
	df.describe()
	df['SocialMedia'].value_counts()
	df.groupby(['Country'] sort=False).sum()
	df.groupby(['Country', 'Hobbyist']).median()
	df.groupby(['Country', 'Hobbyist'])['SocialMedia'].value_counts()
	df.groupby(['Country','Hobbyist']).get_group(('Albania','No')).median()
	df.groupby(['Country','Hobbyist']).get_group(('Afghanistan',
								'No'))[['SocialMedia','Student']].value_counts()
	df.groupby(['Country', 'Hobbyist'])['SocialMedia', 
								'Student'].value_counts().loc[('Afghanistan','No')]
	df.groupby(['Country', 'Hobbyist'])[['ConvertedComp',
								'Age']].agg(['median', 'mean'])
	df.groupby(['Country', 'Hobbyist'])[['ConvertedComp', 'Age']].agg(['median', 
								'mean']).loc[('Afghanistan', 'No')]
	df.groupby(['Country'])['LanguageWorkedWith'].apply(
								lambda x: x.str.contains('Python').sum())
-------------------------------------------------------------------------------------
  1.PODSTAWY GROUPBY([...]):
		**zwraca DataFrameGroupBy, grupuję dane (kolejność) w tabeli według 
		  podanych kolumn (jak tabela przestawna). Można przejść po jego elementach 
		  - kluczami będą pogrupowane kolumny a wartościami wszystkie wiersze w 
		  kolejności tych pogrupowanych kolumn
					df.groupby(['Country'])				
					df.groupby(['Country', 'Hobbyist'])
					df.groupby(['Country', 'Hobbyist'])['ConvertedComp'].median()
									# index	  Country 	  Hobbyist   ConvertedComp 
									#	  ------------------------------------------
									# 4	 |	Afghanistan		No		   1000
									# 6	 |	Afghanistan		Yes		    750
									# 10 |	Afghanistan		Yes		   2300
									# 19 |	Albania			No		   1200
									# 27 |	Albania			Yes			800
				#alternatywa ALE tylko do pogrupowania po jednej kolmnie!!!:
						#df['ConvertedComp'].groupby(df['Country']).sum()
				#z brakiem sortowania w kolumnie grupującej
						#df['ConvertedComp'].groupby(df['Country'], sort=False).sum()
					for keys, values in df.groupby(['Country', 'Hobbyist']):
						print(keys)
						print(values)
									#('Afghanistan', 'No')
									#   index 	Salary 	 Country  		Hobbyist
									# 	  4		 1000     Afghanistan	  No
									#('Afghanistan', 'Yes')
									#   index 	Salary 	 Country  		Hobbyist
									# 	  6		  750     Afghanistan	  Yes
									# 	  10	 2300     Afghanistan	  Yes
	*groups - zwraca słownik krotki kluczy i listy wartości (jej indexów) 
					df.groupby(['Country', 'Hobbyist']).groups
									#{('Afghanistan', 'No'): [4], ('Afghanistan', 'Yes'): [6,10], ('Albania','No') :[19] ...}
					for k, v in df.groupby(['Country', 'Hobbyist']).groups.items():
						print(k)
						print(v)	#('Afghanistan', 'No')
									#Int64Index([4],dtype='int64', name='index')
									#('Afghanistan', 'Yes')
									#Int64Index([6.10],dtype='int64', name='index')
	*get_group() - zwraca DataFrame w postaci przefiltrowanych wierszy z 
		pogrupowanej tabeli. Działa tak samo jak filtr (tylko że wybiera rekordy po pogrupowanej tabeli zamiast przefiltrowywać ją)
					df.groupby(['Country', 'Hobbyist']).get_group(('Albania', 'No'))
									# index	  Country 	    Hobbyist	Salary 
									#	   -------------------------------------
									#  19 |	Albania			No			1200
									#  27 |	Albania			Yes			 800
									# działa tak samo jak:
									# f = (df['Country']=='Albania') & 
									#	 (df['Hobbyist']=='No')
									# df.loc[f]		
-------------------------------------------------------------------------------------
  2.ROZSZERZENIE GROUPBY([...]) - METODY AGREGACYJNE:
	1)WYKORZYSTANIE na NIEZAGREGOWANEJ TABELI:
		a)poszczególnych metod agregacyjnych można użyć na całej tabeli która nie 
		  jest zagregowana. Wtedy automatycznie wybierze tylko kolumny z liczbami 
		  (będzie wycofane) (nie można użyć z metody value_counts() na całej tabeli)
					df.median()						#ConvertedComp    57287.0
													#Age                 29.0
				*describe() - metoda zwracająca podstawowe statystyki na całej 
				 tabeli dla kolumn z liczbami. Zwróci: count (ilość), mean 
				 (średnia), std (standard deviation), min, max, kwantyle: 25%, 50% 
				 (mediana), 75% (można dodać też swoje kwantyle w arg percentiles,
				 można też dodać argument odpowiadjący za typ wyświetlonych danych):				 
					df.describe()	# CompTotal		ConvertedComp		Age
									# count			5.582300e+04		79210.000000
									# mean			1.271107e+05		30.336699
									# std			2.841523e+05		9.178390
									# min			0.000000e+00		1.000000
									# 25%			2.577750e+04		24.000000
									# 50%			5.728700e+04		29.000000
									# 75%			1.000000e+05		35.000000
									# max			2.000000e+06 		99.000000
					df.describe(percentiles = [.20, .55], include=['int']
									# CompTotal			Age
									# count			79210.000000
									# mean			30.336699
									# std			9.178390
									# min			1.000000
									# 20%			20.367400
									# 50%			29.000000
									# 55%			33.848000
									# max			99.000000
		b)poszczególnych metod można użyć na wybranej kolumnie. Wszystkie wartości 
		  NaN zostaną zignorowane podczas obliczeń. 
					df['ConvertedComp'].mean()		#średnia -> 127110.738
					df['Hobbyist'].count()			#liczba wszystkich wystąpień
					df['Hobbyist'].value_counts()	#liczba unikatowych wartości:
													# Yes    71257
													# No     17626
													# Name: Hobbyist, dtype: int64
	2)WYKORZYSTANIE na ZAGREGOWANEJ TABELI:
			**zagregowana tabela też jest DataFrame'em i można użyć na niej metod   
			  agregacyjnych tak sam jak w poprzednim punkcie. 
		a)dla tabeli (nie można korzystać z value_counts())
			**dla całej zaagregowanej tabeli:
					df.groupby(['Country', 'Hobbyist']).median()
									# Country		Hobbyist	ConvertedComp	Age
									# Afghanistan	  No			4464.0		24.5
									#  				  Yes			90000.0		25.0
									# Albania		  No			14352.0		25.0
									#				  Yes			10266.0		24.0 
			**dla wybranego klucza w zaagregowanej tabeli:
					df.groupby(['Country','Hobbyist']).get_group(
														('Albania','No')).median()
									# ConvertedComp     14352.0		
									# Age                 25.0
									# dtype: float64
		b)dla poszczególnej kolumny/kilku kolumn -> [['SocialMedia', 'Student']] :
			**dla kolumny z całej zaagregowanej tabeli:
					df.groupby(['Country', 'Hobbyist'])['SocialMedia'].value_counts()
									# Country       Hobbyist  SocialMedia   
									# Afghanistan     No        Facebook       6
									#  				 		    YouTube        2
									#						    WhatsApp       2
									#				  Yes       Facebook       7
									#				  		    YouTube        7
									# Albania         No        Facebook       5
									#				  Yes       WhatsApp       14
									#			 	 		    Facebook       11
									#							Instagram      10
			**dla wybranej kolumny/kolumn ( [['SocialMedia', 'Student']] ) w 
			  zaagregowanej tabeli dla wybranego klucza (('Afghanistan','No')):
					df.groupby(['Country','Hobbyist']).get_group(('Afghanistan',
									'No'))[['SocialMedia','Student']].value_counts()
									# będzie to samo co gdybyś użył:
									# filt = (df['Country'] == 'Afghanistan') & 
									#					(df['Hobbyist'] == 'No')
									# df.loc[filt]['SocialMedia'].value_counts()
									# Facebook    No                4
												  Yes			    2
									# YouTube     No                1
												  Yes			    1
									# WhatsApp    Yes				2
			**dla wybranej kolumny/kolumn w zaagregowanej tabeli ALE ze wskazanym 
			  kluczem na samym końcu poprzez atrybut loc['columnName'] :
					df.groupby(['Country', 'Hobbyist'])[['SocialMedia', 
								'Student']].value_counts().loc[('Afghanistan', 'No')]
									# będzie to samo co gdybyś użył:
									# filt = (df['Country'] == 'Afghanistan') & 
									#					(df['Hobbyist'] == 'No')
									# df.loc[filt]['SocialMedia'].value_counts()
									# lub get_group()
									# df.groupby(['Country','Hobbyist']).get_group(
									# ('Afghanistan','No'))[['SocialMedia',
									# 'Student']].value_counts()
									# Facebook    No                4
												  Yes			    2
									# YouTube     No                1
												  Yes			    1
									# WhatsApp    Yes				2
-------------------------------------------------------------------------------------
  3.GRUPOWANIE kolumn ZAMIAST WIERSZY:
	*https://www.geeksforgeeks.org/combining-multiple-columns-in-pandas-groupby-with-dictionary/
					import pandas as pd
					dict = {"Movies":["The Godfather", "Fight Club", "Casablanca"],
							"Week_1_Viewers":[30, 30, 40],
							"Week_2_Viewers":[60, 40, 80],
							"Week_3_Viewers":[40, 20, 20] };
					df = pd.DataFrame(dict);
					groupby_dict = {"Week_1_Viewers":"Total_Viewers",
							"Week_2_Viewers":"Total_Viewers",
							"Week_3_Viewers":"Total_Viewers",
							"Movies":"Movies" }
					df2 = df.groupby(by=groupby_dict, axis = 1).sum()	
									#  	    Movies   	 Total_Viewers
									# 0  The Godfather        130
									# 1    Fight Club          90
									# 2    Casablanca         140					
-------------------------------------------------------------------------------------
  4.ROZSZERZENIE ZAAWANSOWANE GROUPBY([...]):
	0) count() VS value_counts() 
			df['Hobbyist'].count()			#liczba wszystkich rekordów w kolumnie
			df['Hobbyist'].value_counts()	#liczba każdego unikalnego rekordu w col.
	1) dla funkcji value_counts() można dodać atrybut normalize = True dzięki czemu 
	   zostaną zwrócone procenty -> value_counts(normalize=True)
					df.groupby(['Country','Hobbyist'])['SocialMedia','Student'].
							value_counts(normalize=True).loc[('Afghanistan', 'No')]
									# Facebook    No                0.4
												  Yes			    0.2
									# YouTube     No                0.1
												  Yes			    0.1
									# WhatsApp    Yes				0.2
	2) metoda agg() / aggregate() - wiele metod agregacyjnych dla jednego zapytania 
	   (value_counts nie można używać z innymi metodami agregacyjnymi):
					import numpy
					df.groupby(['Country', 'Hobbyist'])[['ConvertedComp', 
												'Age']].agg([numpy.median, 'mean'])
						# Country	  Hobbyist		ConvertedComp		  	 Age
						# 						median		mean		median	mean
						# Afghanistan	No		4464.0		4148.00		24.5	29.37
						# 				Yes		14364.0		134555.11	25.0	27.13
						# Albania		No		14352.0		23174.60	25.0	27.07
										Yes		10266.0		21498.47	24.0	24.72
					df.groupby(['Country', 'Hobbyist'])[['ConvertedComp', 
						  'Age']].agg(['median', 'mean']).loc[('Afghanistan', 'No')]
					df.groupby(['Country', 'Hobbyist'])[['SocialMedia',
													'Student']].agg('value_counts')	
		**różne metody agregacyjne dla różnych kolumn:
					df.groupby(['Country', 'Hobbyist'])[['ConvertedComp', 
							'Age']].agg({'ConvertedComp':'median', 'Age':'mean'])
						# Country	  Hobbyist	 ConvertedComp		Age
						# 							median			mean
						# Afghanistan	No			4464.0			29.37
						# 				Yes			14364.0			27.13
						# Albania		No			14352.0			27.07
										Yes			10266.0			24.72
	3) metoda transform() - pozwala na obliczenia na zgrupowanych rekordach gdzie 
	   można wykorzystac funkcje agregujące:
					df.groupby(['Country', 'Hobbyist'])[['ConvertedComp', 
							'Age']].transform(lambda x: x-x.mean()/x.std())
						# Country	  Hobbyist	 ConvertedComp		Age
						# Afghanistan	No			 -345.0			-34
						# 				Yes			   25.3			 23
						# Albania		No			  752.0			 12
										Yes			  987.8			 69
	4) metoda filter() - pozwala filtrować zgrupowane dane:
					df.groupby(['Country']).filter(lambda x: len(x) >4)
						#wyświetli wszystkie dane grupowane po krajach gdzie dany 
						#kraj występuję więcej niż 4 razy						
	5) str.contains - można też skorzystać z metody string sprawdzającej czy coś 
	   jest zawarte w każdej komórce --> wtedy musisz użyć metody apply gdyż 
	   pracujesz na DANYCH TYPU: SeriesGroupBy a nie Series:
					df.groupby(['Country'])['LanguageWorkedWith'].apply(
										lambda x: x.str.contains('Python').sum())
									# używasz sum() zamiast count() żeby zsumowało 
									# Tobie wszystkie pasujące wyniki (True jako 1)
									# zamiast count() by policzyło Tobie wszystkie 
									# elementy dla każdej grupy
									#						Country
									#						Afghanistan   	8
									#						Albania    	  	23
									#						Algeria			40
									#						Andorra 		0
	6) przedstawienie wyników - wykorzystanie pd.concat():
			*utworzenie dwóch zmiennych typu Series (poprzez pobranie danych):
					country_residents = df['Country'].value_counts()
					country_uses_python=df.groupby(['Country'])['LanguageWorkedWith'
								].apply(lambda x: x.str.contains('Python').sum())
			*połączenie dwóch tabel typu Series (pierwsza tabela przejmie nazwę 
			 drugiej kolumny 'LanguageWorkedWith'):
					#import pandas as pd
					python_df = pd.concat([country_residents, country_uses_python], 
														axis='columns', sort=False)
			*zmiana nazw kolumn:
					python_df.rename(columns={'Country':'NumRespondents', 
							'LanguageWorkedWith':'NumKnowsPython'}, inplace = True)
			*dołożenie kolumny: 
					python_df['pctKnowsPython'] = (python_df['NumKnowsPython']/
														python_df['NumRespondents'])
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------

2.MODYFIKACJA KOMÓREK:
  1)PODSTAWY:
	 **PODMIANA WARTOŚCI - robisz poprzez przipsanie lub metody:
			*nazwy kolumn:
					df.columns = ['first', 'last', 'email']
					df.rename(columns={'first':'First name'}, inplace=True)
			*Wartości:
					df.loc[2, ['First name', 'email']] = ['Matty', 'ms@gmail.com']
					df.loc[(df['email'] == 'ms@gmail.com'), 'last'] = 'Gato'
					#plus metody replace i map
	 **WYWOŁANIE METODY NA WSZYSTKICH KOMÓRKACH W DANEJ KOLUMNIE:
			*na Stringach:
					df['Symbol'].str.lower()				#str.lower()
					df['Symbol'].apply(str.lower)			#apply
					df['Symbol'].apply(lambda x: x.lower())	#apply i lambdy
			*na Datach (nie działa opcja apply(dt.day_name)):
					df['Date'].dt.day_name()
					### df['Date'].apply(dt.day_name) (NIE DZIAŁA!)
					df['Date'].apply(lambda x: x.day_name())
	 **UŻYWANE METODY --> (Będziesz przede wszystkim korzystał z APPLYMAP do funkcji 
	   na danych i REPLACE do podmiany danych --> patrz pkt4):
			*wywołanie funkcji na danych:
				  APPLYMAP -> będącymi obiektami DateFrame
				  APPLY    -> będącymi obiektami Series
			*podmiana wartość w komórkach na obiektach Series:
				  MAP      -> wartości których nie ma jako klucze w słowniku 
				  			  zostają podmienione na 'NaN'
				  REPLACE  -> wartości których nie ma jako klucze w słowniku 
							  zostają NIEZMIENIONE
  2)DODAWANIE/USUWANIE KOLUMN, WIERSZY, TABEL:
	A)KOLUMNY:
		a)Dodanie nowej kolumny:
			**z wszystkimi komórkami w kolumnie o jedej wartości
					df = df.assign(my_pro='NAN')
			**z każdą komórką w kolumnie o różnej wartości (musi być dokładnie tyle 
			  komórek ile jest indexów - inaczej błąd)
					df['address'] = [4000, 20000, 30000]			// LUB //
					df = df.assign(my_profit=[4000, 20000, 30000])
								# 	first	last	email		  my_profit
								# 0	Marek	Dziak	md@gmail.com  4300
								# 1	Anna	Nacz	an@gmail.com  20000
								# 2	Lukas	Lipka	ll@gmail.com  30000
			**zainsertowanie kolumy w konkretnym miejscu:
					df.insert(2, "Age", [21,22,23])
								# 	first	last	age		email	    my_profit
								# 0	Marek	Dziak	 21	  md@gmail.com  4300
								# 1	Anna	Nacz	 22   an@gmail.com  20000
								# 2	Lukas	Lipka	 23   ll@gmail.com  30000	
		b)Dodanie kolumny na bazie innych kolumn (nie wolno używać notacji z kropką)
					df['fullname'] = df['first']+' '+ df['last']  # NIE df.fullname
					df['fullname'] = df[['first', 'last']].apply(
													lambda x: ' '.join(x), axis = 1)
			**str.cat(others=None ,sep=None ,na_rep=None) - dodanie kolumny do 
			  kolumny poprzez konktatenacje stringów z kolumny do kolumny:
					df["Name"]= df["Name"].str.cat(others=df["Team"].copy(), 
													sep =", ", na_rep="No name")
								#do elementów kolumny df["Name"] dopisze kolumne 
								#df["Team"]. Gdy w df["Name"] pojawi sie NULL to 
								#zamieni go na napis "No name":
								#					"marek	, Boston Celtics" 
								#					"No name, Chicago Bulls"
		c)Podział kolumny na inne kolumny:
					df[['first','last']] = df['fullname'].str.split(' ',expand=True)
		d)Usuwanie kolumny/kolumn (axis = 1)
			**drop:
					df.drop(columns=['first','last'], inplace = True)
					df.drop(['first','last'], axis=1, inplace = True)
					df.drop(df.columns[[0, 4, 2]], axis=1, inplace=True) 
											#zostaną kolumny 'last' i 'email'
					df.drop(df.iloc[:, 1:4], axis=1, inplace=True) 
											#zostaną kolumny 'first' i 'my_profit'
					df.drop(df.loc[:, 'last':'email'], axis=1, inplace=True) 
											#zostaną kolumny 'first' i 'my_profit'	
	B)WIERSZE:
		a)Dodanie wiersza 
			**komórki dla kluczy które nie zostaną wprowadzone dostaną wartość NaN
			**klucz który nie będzie istnieć zostanie dodany jako nowa kolumna
			**dla Dataframów są 2 metody - concat i append (append będzie wycofane)
					df = df.append({'first':'Johny'}, ignore_index=True)
					df = pd.concat([df, pd.DataFrame([{'first':'Al', 
											'last':'Geno'}])],ignore_index=True)
			**dodanie wiersza będącego elementem Series:
					df1= pd.DataFrame(people ,index=[0, 1, 2])
					s1 = pd.Series([4300, 20000,30000], name='my_profit')
					df3 = pd.concat(df1, s1, axis=1)
								# 	first	last    my_profit
								# 0	Marek	Dziak     4300
								# 1	Anna	Nacz	  20000
								# 2	Lukas	Lipka	  30000	
		b)Usuwanie wiersza:
			**drop:
				**na podstawie indexu (błąd jeżeli index/indexy nie istnieją):
					df.drop(index=3, inplace=True)
					df.drop(index=[1,2], inplace=True)
				**na podstawie przefiltrowanych danych: 
					filt = df['last']=='Dziak'
					df.drop(index=df[filt].index, inplace=True)
			**truncate (żeby móc tego użyć to wpierw index musi być posortowany):
					df.sort_index(inplace=True)
										# 	first	last	email			profit
										# 0	Anna	Nacz	an@gmail.com	NaN
										# 1	Ewela	Mus		NaN				400.0
										# 2	Kal		Dziak	NaN				399.0	
										# 3	Lukas	Lipka	ll@gmail.com	NaN
										# 4	Marek	Dziak	md@gmail.com	NaN
					df = df.truncate (inplace=True, before = 'Kal', after ='Lukas')
										# 	first	last	email			profit
										# 2	Kal		Dziak	NaN				399.0	
										# 3	Lukas	Lipka	ll@gmail.com	NaN
	C)TABELE - dodanie całej tabeli do tabeli:
		x)APPEND() (ma zostać wycofane):
					df = df.append(df2, ignore_index=True, sort=False)
		a)pd.CONCAT() / concat():
			**argument ignore_index = True - indexy zostaną zignorowane a komórki 
			  dla nie_tych_samych kolumn w tabelach dostaną wartość NaN:
					df = pd.concat([df, df2], ignore_index=True)
										# 	first	last	email			profit
										# 0	Marek	Dziak	md@gmail.com	NaN
										# 1	Anna	Nacz	an@gmail.com	NaN
										# 2	Lukas	Lipka	ll@gmail.com	NaN
										# 3	Ewela	Mus		NaN				400.0
										# 4	Kal		Dziak	NaN				399.0
			**argument axis
				*axis=0 - dodawanie tabel poprzez dodanie wierszy nawet jeśli 
				 istnieją te same indeksy.
				*axis=1 - dodawanie tabel poprzez dodanie kolumn i łączenie 
				 wierszy o tych samych indeksach.
			**argument join (razem z argumentem axis=1):
					people = {
						"first": ['Marek', 'Anna', 'Lukas'],
						"last": ['Dziak', Nacz, 'Lipka']
					}
					additionals = {
						"first": ['Anna', 'Lukas','Darek'],
						"email": ['an@gmail.com','ll@gmail.com','ki@gmail.com'],
						"age": [33, 34, 45]
					}
					df1 = pd.DataFrame(people, index =[0,1,2])
					df2 = pd.DataFrame(additionals, index = [1,2,3])
			  **join ='outer' (domyślnie) - wiersze o tych samych indeksach 
				zostaną połączone a reszta NIEZOSTANIE usunięta:
					df3 = pd.concat([df1,df2], axis=1)		 	   #join='outer'
								# 	first	last	first	email			age
								# 0	Marek	Dziak	NaN		NaN				NaN
								# 1	Anna	Nacz	Anna	an@gmail.com	33
								# 2	Lukas	Lipka	Lukas	ll@gmail.com	34
								# 3	NaN		NaN		Darek	ki@gmail.com	45
			  **join='inner' - wiersze o tych samych indeksach zostaną 
				połączone a reszta ZOSTANIE usunięta:
					df3 = pd.concat([df1,df2], axis=1, join='inner')
								# 	first	last	First	email			age
								# 1	Anna	Nacz	Anna	an@gmail.com	33
								# 2	Lukas	Lipka	Lukas	ll@gmail.com	34
		b)join() - łączenie tabel TYLKO po domyślnych indeksach (nie pozwala na 
		  łączenie tabel o tych samych nazwach kolumn nawet jeżeli sa ustawione 
		  na indeksy)(nie pozwala na kolumny o tych samych nazwach w 2 tab)
			**argument how = ['left', 'rigth', 'inner', 'outer'] - tak jak w sql
			  można określić jak mogą się łączyć tabele (domyślnie inner) 
			  (użyte w poniższych przykładach)
					people = {
						"first": ['Marek', 'Anna', 'Lukas'],
						"last": ['Dziak', 'Nacz', 'Lipka']
					}
					additionals = {
						"email": ['ma@gmail.com','xx@gmail.com','yy@gmail.com'],
						"age": [33, 34, 45]
					}
					df1 = pd.DataFrame(people)
					df2 = pd.DataFrame(additionals, index = [0,7,8])
					df3 = df1.join(df2, how='inner')
								#	first	last	email		  age
								#0	Marek	Dziak	ma@gmail.com  33
			**argument on - w pierwszej tabeli musi być domyślny index a w 
			drugiej (wewnątrz funkcji join) indexy ustawiasz poprzez odpowiednie 
			nazwy (nie wolno ustawić indeksu na poszczególna kolumnę). W arg
			on wskazujesz kolumne będącą kluczem ocym dla klucza głównego 
			(indexu) w drugiej tabeli. Nie można zastosować w drugiej tabeli 
			funkcji set_index():
					people = {
						"first": ['Mar', 'An', 'Lukas'],
						"last": ['Dziak', 'Nacz', 'Lipka']
					}
					additionals = {
						"email": ['an@gmail.com','ll@gmail.com','ki@gmail.com'],
						"age": [33, 34, 45]
					}
					df1 = pd.DataFrame(people)
					df2 = pd.DataFrame(additionals, index = ['Mar', 'An', 'D'])
					df3 = df1.join(df2, on='first', how='outer')	
								# 		first	last	email			age
								# 0.0	Mar		Dziak	an@gmail.com	33.0
								# 1.0	An		Nacz	ll@gmail.com	34.0
								# 2.0	Lukas	Lipka	NaN				NaN
								# NaN	Da		NaN		ki@gmail.com	45.0
		c)pd.MERGE() - łączenie tabel po kluczach będącymi osobnymi kolumnami 
					***nie pozwala lączyć tabel po indeksach (do indeksów join())
					***powala na kolumny o tych samych nazwach w 2 tabelach.
			**argument how = ['left', 'rigth', 'inner', 'outer'] - tak jak w sql
			  zostało rozpisane w join() powyżej)(użyte w poniższych przykładach)
			**argument on - klucze muszą mieć takie same nazwy kol w 2 tabelach:
					data1 = {'key_P': ['K0', 'K1', 'K2'],
							 'Name':['Jai', 'Princi', 'Gaurav']
							}
					data2 = {'key_P': ['K0', 'K1', 'K2', 'K3'],
							 'Address':['Nag', 'Kan', 'Alla', 'Bannu']
							}
					df1 = pd.DataFrame(data1)
					df2 = pd.DataFrame(data2) 
					pd.merge(df1, df2, how='inner', on='key_P')								
										# 	key_P	Name	Address
										# 0	K0		Jai		Nag
										# 1	K1		Princi	Kan
										# 2	K2		Gaurav	Alla	
			**argumenty left_on, right_on - pozwalają określić klucze (będące 
			  odpowiednimu kolumnami)
					data1 = {'key_P': ['K0', 'K1', 'K2'],
							 'Name':['Jai', 'Princi', 'Gaurav']
							}
					data2 = {'key_P': ['s0', 's1', 's2', 's3'],
							 'Address':['Nag', 'Kan', 'Alla', 'Bannu'],
							 'key_F': ['K0', 'K0', 'K2', 'K6'],
							}
					df1 = pd.DataFrame(data1)
					df2 = pd.DataFrame(data2) 
					pd.merge(df1, df2, how='right', 
											left_on='key_P', right_on='key_F')
									#   key_P_x Name  key_P_y  Address  key_F
									# 0	K0		Jai		s0		Nag		K0
									# 1	K0		Jai		s1		Kan		K0
									# 2	K2		Gaurav	s2		Alla	K2
									# 3	NaN		NaN		s3		Bannu	K6	
			****można użyć kilku kolumn jako klucze:
					data1 = {'key_P1': ['K0', 'K1', 'K2'],
							 'key_P2': ['L0', 'L1', 'L2'],
							 'Name':['Jai', 'Princi', 'Gaurav']
							}
					data2 = {'key_P': ['s0', 's1', 's2', 's3'],
							 'Address':['Nag', 'Kan', 'Alla', 'Bannu'],
							 'key_F1': ['K0', 'K0', 'K2', 'K6'],
							 'key_F2': ['L0', 'L6', 'L2', 'K9']
							}
					df1 = pd.DataFrame(data1)
					df2 = pd.DataFrame(data2)	
					pd.merge(df1,df2, how='left',left_on=['key_P1','key_P2'], 
												 right_on=['key_F1','key_F2'])
					# 	key_P1	key_P2	Name	key_P	Address	key_F1	key_F2
					# 0	K0		L0		Jai		s0		Nag		K0		L0
					# 1	K1		L1		Princi	NaN		NaN		NaN		NaN
					# 2	K2		L2		Gaurav	s2		Alla	K2		L2
			****zamiast pd.merge() możesz użyć na danej tabeli funkcji merge()
					df3 = df1.merge(df2, how='outer', 
											left_on='key_P', right_on='key_F')
									#	key_P_x	Name	key_P_y	Address	key_F
									# 0	K0		Jai		s0		Nag		K0
									# 1	K0		Jai		s1		Kan		K0
									# 2	K1		Princi	NaN		NaN		NaN
									# 3	K2		Gaurav	s2		Alla	K2
									# 4	NaN		NaN		s3		Bannu	K6
	D)INNE:
		a)dodawanie SERIES do SERIES:
								sr1 = pd.Series(['New York', 'Chicago'])
								sr2 = pd.Series(['Toronto', 'Lisbon'])
								sr1.append(sr2, ignore_index=True)
								# 0    New York
								# 1     Chicago
								# 2     Toronto
								# 3      Lisbon
								# dtype: object
		b)dodawanie INDEXÓW do INDEXÓW:
								df1 = pd.Index(['Jan', 'Feb', 'Mar', 'Apr'])
								df2 = pd.Index(['May', 'Jun', 'Jul', 'Aug'])
								df3 = pd.Index(['Sep', 'Oct', 'Nov', 'Dec'])
								df1.append([df2, df3])
								#Index(['Jan', 'Feb', 'Mar', 'Apr','May', 'Jun', 
								#		'Jul', 'Aug','Sep', 'Oct', 'Nov', 'Dec'])
  3)CZYSZCZENIE DANYCH z NaN (np.nan) i None oraz ZMIANA TYPU WARTOŚCI:
					import pandas as pd
					import numpy as np
					people = {
						"first": ['Marek', 'Anna', 'Lukas'],
						"last": ['Dziak', np.nan, 'Lipka'],
						"email": ['md@gmail.com',None,'ll@gmail.com'],
						"age": [33, 34, 45]
					}
					df = pd.DataFrame(people)
		A)SPRAWDZENIE CZY w TABELI znajdują się: np.nan/None:
					df.isna()  /  df.isnull()
										#	first	last	email	age
										# 0	False	False	False	False
										# 1	False	True	True	False
										# 2	False	False	False	False
					df.notnull() 	
										#	first	last	email	age
										# 0	True	True	True	True
										# 1	True	False	False	True
										# 2	True	True	True	True
		B)SPRAWDZENIE CZY w KOLUMNIE znajdują się: np.nan/None:
					filt = pd.isna(df['last'])  /  filt = pd.isnull(df['last'])
										# 0	 False
										# 1	 True
										# 2  False
					df[filt]	#tylko te wiersze gdzie w kolumnie był nan/None
										# 	first	last	email			age
										# 1	Anna	Nacz	an@gmail.com	34
					filt = pd.notnull(df['last'])
										# 0	 True
										# 1	 False
										# 2  True
					df[filt]	#tylko te wiersze gdzie w kolumnie nie było nan/None
										# 	first	last	email			age
										# 0 Marek	Dziak	md@gmail.com	33
										# 2	Lukas	Lipka	ll@gmail.com	45
		C)USUNIĘCIE WIERSZY(INDEXÓW)/KOLUMN gdzie w jakiejkolwiek/we wszystkich 
		  kolumnach jest np.nan/None w podanych kolumnach (subset)
		  dropna(axis ='index/rows/columns', how='any/all', subset=['col1','col2']) 
		  
					df.dropna()  	/ 	 df.dropna(axis='index', how='any')
					df.dropna(axis='rows', how='any') 	 #gdy w jakiejkolwiek komórce 
														 #w wierszu jest NaN / None 
														 #to wiersz zostaje usunięty
					df.dropna(axis='columns', how='all') #gdy we wszystkich komórkach
														 #w kolumnie jest NaN / None
														 #to kolumna zostaje usunięta
					df.dropna(axis='rows', how='any', subset=['first', 'last'])
														 #gdy w jakiejkolwiek komórce
														 #w kolumnach 'first'/'last'
														 #pojawi się NaN / None to 
														 #wiersz zostaje usunięty
		D)ZAMIANA wartości np.nan/None na Strin lub odwrotnie:
					import numpy as np
					df.replace(to_replace=np.nan, value='missing', inplace=True)
					df.replace(np.nan, 'missing', inplace=True)
		E)WYPEŁNIENIE wartości NaN/None na inną wartość:
			*zamiana np.nan/None tylko w kolumnach:
					df['email'].fillna('M', inplace=True)
			*zamiana np.nan/None na podaną wartość w całej tabeli:
					df.fillna(0, inplace=True)
			  **zamiana na wart. poprzednie('pad') w kolumnie - NaN'y w 1szym 
				wierszu nie zosaną wypełnione
					df.fillna(method='pad', inplace = True)
			  **zamiana na wart. następne('bfill') w kolumnie - NaNy w ostat. 
				wierszu nie zostaną wypełnione
					df.fillna(method='bfill', inplace = True)
		F)SPRAWDZENIE TYPÓW WARTOŚCI i ZAMIANA TYPÓW WARTOŚCI - NaN jest domyślnie
		  typem float i w funkcjach agregacyjnych przy obliczeniach nie jest brany pod uwagę ALE by móc skorzystać z tych funkcji agregacyjnych to inne dane w kolumnie też muszą być typem float i NIE MOGĄ BYĆ np int (intigerem)
			  **zwrot typów wartości dla danej kol (Object - Stringi/dane mieszane)
					df.dtypes
			  **zwrot wszystkich unikalnych wartośći w danej kolumnie
					df['YearsCode'].unique()
			  **zamiana typu wartości:
					df['YearsCode'] = df['YearsCode'].astype(float)
					df['YearsCode'].mean()				 #teraz można użyć mean()
  4)MODYFIKACJE TEKSTU:	
		*str.lower(), str.upper(), str.title(), str.strip()/rstrip()/lstrip():
			*zapis tylko dla 1 kolumny - działa tylko dla Series a nie na DataFrame!
				*str.function()
					df["Name"] = df["Name"].str.lower() 	
					df.loc[[1,2,3],"Name"] = df.loc[[1,2,3],"Name"].str.upper()
					df.loc[1:3,"Name"] = df.loc[1:3,"Name"].str.title() #1 litera
				*apply(str.function) / apply(lambda x: x.function())
					df.loc[1:1,"Name"].apply(str.rstrip) 	#zapis dla 1 komórki 
					df['Name'].apply(lambda x: x.strip())	
			*zapis dla obiektów DataFrame:
					df.loc[[1,2,3],["Name", "Address"]].applymap(str.capitalize)
					df.loc[:,["Name", "Address"]].applymap(lambda x: x.capitalize)
		*str.replace() - zamiana wartości w obiektach Series:
			*str.replace() działa tylko dla obiektów Series które sa Stringami a 
					df["Address"].str.replace("Delhi", "Miami")
			*obj_series.replace() / obj_dataframe.replace() - działa dla obiektów 
			 o wszystkich typach czyli Series i DataFrame:
					df["Age"].replace(24.0, "Twenty four")
					df.loc[[2,3],["Age", "Name"]].replace("Anuj","xx")
					df.replace(to_replace=["Boston", "Texas"], value="Omega")
		*str.cat(others=None ,sep='my_str', na_rep='my_str')- konkatenacja Stringów 
		 w Series - nie mogą byc to inne typy wartości niż String ani inne obiekty 
		 niż Series:
					df["Name"]= df["Name"].str.cat(df["Address"].copy() , sep =", ")
														# 0            Jai, Delhi
														# 1        Princi, Kanpur
														# 2    Gaurav, Allahabada
		*str.join() -  działa tylko na obiektach Series:
			*gdy Series składa sie ze Stringów to wrzuca łącznik pomiędzy litery:
					data["Name"]= data["Name"].str.join("-")
														# 0    A-v-e-r-y- -B-r-a-d
														# 1    G-e-o-r-g- -B-o-s-h
			*gdy obiekt Series zawiera elementy będące listami to str.join() złączy
			 elemety tej listy podanym łącznikiem. 
					data["Team"]= data["Team"].str.split("t")
														#zwróci Series list: 
														# 0 ['Bos', 'on Cel','ics']	
														# 1 ['Bos', 'on Cel','ics']	
					data["Team"]= data["Team"].str.join("_")
														# 0 'Bos_on Cel_ics'
														# 1 'Bos_on Cel_ics'
		*str.split() (przykład dla 'Allachabada'):
			*rozdziela tylko po literach wewnątrz napisu (nie patrzy na 1 literę):
					df.loc[1:1,"Address"].str.split("a")		  #[All, h, b, d,]
			*w argumencie 'n' można podać maksymalną ilośc wystąpień	
					df.loc[1:1,"Address"].str.split("a, n=2)	  #[All, h, bad]
			*dla arg. expand=True - zwróci obiek Series:
					df.loc[1:1,"Address"].str.split("a", n=2, expand=True)
																  #   0	  1	2
																  # 2 All h	bada
				*chcąc zrobić przypisanie to można to zrobić tylko dla całych 
				 kolumn i to bez używania .loc[] / .iloc. Ponadto trzeba przypisac 
				 do takiej liczby kolumn jaka liczba podzielonych tekstu w 
				 najbardziej licznym przypadku 
					df[["Ad1","Ad2","Ad3","Ad4","Ad5"]] = df["Address"].str.split(
																  "a", expand=True)
					#	Name 	Address		Ad1		Ad2		Ad3		Ad4		Ad5
					# 0	Jai		Delhi		Delhi	None	None	None	None
					# 1 Gaurav	Allahabada	All		h		b		d	
					# 2	Anuj	Kannauj		K		nn		uj		None	None
		*str.extract() - działa tylko na obiektach Series. Wewnątrz podajesz REGEXa. 
		 Gdy Regex jest bez grup to zwróci obiekt Series a dla kilku grup stworzy 
		 DataFrame'a gdzie nazwy kolumn będą przejęte od nazw grup REGEXa.
					s = pd.Series(['a1', 'b2', 'c3'])
					s.str.extract(r'(?P<Geeks>[ab])(?P<For>\d)')
														#  	 Geeks  For
														# 0     a    1
														# 1   NaN  NaN
														# 2   NaN  NaN					
  5)METODY:	
	*APPLYMAP()
	    a*wywołuję funkcję na wybranych danych
		b*działa na obiektach klasy DataFrame, nie działa na obiektach klasy Series
		c*NA CAŁEJ TABELI - każda komórka w tabeli zostanie zmieniona 
					df = df.applymap(str.lower)   		/ 
					df.loc[:,:] = df.loc[:,:].applymap(str.lower)
														# 	first  last  email
														# 0 marek  dziak md@gmail.com
														# 1 anna   nacz  an@gmail.com
														# 2 lukas  lipka ll@gmail.com
		d*można wybrać całą kolumnę (lub kilka) lub cały wiersz (lub kilka) lub 
		  komórkę w tabeli (lub kilka) ale trzeba użyć specjalnej składni w loc[] 
		  zawierającej nawiasy[] wewnątrz nawiasów[]:
					df.loc[:,['email']] = df.loc[:,['email']].applymap(str.upper)
														#df['email'] - nie zadziała 
														#	email
														# 0	MD@GMAIL.COM
														# 1	AN@GMAIL.COM
														# 2	LL@GMAIL.COM
					df.loc[[2],:] = df.loc[[2],:].applymap(labda x: x.upper())
														# 	first  last  email
														# 2 LUKAS  LIPKA LL@GMAIL.COM
					df.loc[[0],['email']] = df.loc[[0],['email']].applymap(str.upper)
														# 	 email		
														# 0 MD@GMAIL.COM	
			  ***również zadziałą na przefiltrowanych danych ale trzeba pamiętajać
				 o specjalnej składni podwójnych nawiasów[]
					def my_update(email):
						return email.upper()
					filt = (df['last']=='Dziak')
					df.loc[filt,['email']].applymap(my_update)
	*APPLY()
		a*wywołuję funkcję na wybranych danych
		b*działa na obiektach klasy Series, nie działa na obiektach klasy DataFrame  
		c*Działa tylko na POJEDYŃCZYCH kolumnach i POJEDYŃCZYCH wierszach. 
		d*Nie działa na POJEDŃCZYCH komórkach chyba że zostaną one przedstawione
		  w formie Series: df.loc[0,['first']] zamiast df.loc[0,'first']. Nie można 
		  wybrać kilku komórek (nawet w jednej kolumnie) bo będzie to już DataFrame) 
					df.loc[:,'email'] = df.loc[:,'email'].apply(str.lower) 
					df['email'] = df['email'].apply(str.upper)
											#.apply(lambda x: x.upper())
											#.str.upper()
														# 0    MD@GMAIL.COM
														# 1    AN@GMAIL.COM
														# 2    LL@GMAIL.COM
														# Name: email, dtype: object
					df.loc[0] = df.loc[0].apply(str.lower)
										#.apply(lambda x: x.lower())
										#.str.lower()
														# first    marek
														# last     dziak
														# email    md@gmail.com
														# Name: 0, dtype: object
					df.loc[0,['first']] = df.loc[0,['first']].apply(str.lower)
														# first    marek
														# Name: 0, dtype: object
			  ***również działa na przefiltrowanej kolumnie ale musi to by obiekt
				 Series (nie może być podwójnego nawiasu[] w miejscu kolumny):
					def my_update(email):
						return email.lower()
					filt = (df['last'] == 'Dziak') 
					df.loc[filt, 'email'] = df.loc[filt,'email'].apply(my_update)
														# 0    md@gmail.com
														# Name: email, dtype: object
		c*zadziała też na całej tablicy ale będącej w formie obiektu Series (działa
		  na wszytkich kolumnach realizując funkcje na każdej kolumnie z osobna)
					df.apply(lambda x: x.min())			# wybierze najmniejszą
														# wartośc z każdej kolumny
														# first    Anna
														# last     Dziak
														# email    an@gmail.com
														# dtype: object
	*MAP():
		a*pozwala zamienić wartości komórek (traktując je jako klucze w słowniku) 
		  na innne wartości (będące wartościami dla kluczy w słowniku)
		b*resztę wartości komórek które nie wystepują jako klucze w słowniku funkcja 
		  map() zmienia na 'NaN'
		c*działa tylko dla obiektów typu Series - pozwala zamieniać wartości tylko 
		  dla pojedyńczej kolumny, pojedyńczego wiersz lub pojedyńczej komórki 
		  (gdzie trzeba pamiętać o podwójny nawiasie[] ALE tylko w miejscu kolumny):
					df.loc[:,'last'] = df.loc[:,'last'].map({'Dziak':'Czak'})   / 
					df['last'] = df['last'].map({'Dziak':'Czak'})
														 #  0     Czak
														 #	1     NaN
														 #	2     NaN
														 #	Name: last, dtype: object
					df.loc[0] = df.loc[0].map({'Dziak':'Czak','Jozek':'Wark'})
														 # first     Wark
														 # last      Czak
														 # email     NaN
														 # Name: 0, dtype: object
					df.loc[0,['last']] = df.loc[0,['last']].map({'Dziak':'Czak'})  
														 # last      Czak
														 # Name: 0, dtype: object
			  ***również działa na przefiltrowanej kolumnie ale musi to by obiekt
				 Series (nie może być podwójnego nawiasu[] w miejscu kolumny):
					filt = (df['email'] == 'md@gmail.com') 
					df.loc[filt, 'last'] = df.loc[filt,'last'].map({'Dziak':'Czak'})
														 # 0    Czak
														 # Name: last, dtype: object
	*REPLACE():
		a*pozwala zamienić wartości komórek (traktując je jako klucze w słowniku) 
		  na innne wartości (będące wartościami dla kluczy w słowniku)
		b*resztę wartości komórek które nie wystepują jako klucze w słowniku ZOSTAJE 
		  TAKA SAMA - funkcja replace() nie zamienia na 'NaN'.
		c*działa tylko dla obiektów typu Series - pozwala zamieniać wartości tylko 
		  dla pojedyńczej kolumny, pojedyńczego wiersz lub pojedyńczej komórki 
		  (gdzie trzeba pamiętać o podwójny nawiasie[] ALE tylko w miejscu kolumny):
					df.loc[:,'last'] = df.loc[:,'last'].replace({'Dziak':'Czak'})
					df['last'] = df['last'].replace({'Dziak','Czak'})
														 # 0     Czak
														 # 1     Nacz
														 # 2    Lipka
														 # Name: last, dtype: object
					df.loc[0] = df.loc[0].replace({'Marek':'Garek', 'Dziak':'Czak'})
														 # first    Garek
														 # last     Czak
														 # email    md@gmail.com
														 # Name: 0, dtype: object
					df.loc[0,['last']] = df.loc[0,['last']].replace({'Dziak':'Czak'})
														 # last     Czak
														 # Name: 0, dtype: object
			  ***również działa na przefiltrowanej kolumnie ale musi to by obiekt
				 Series (nie może być podwójnego nawiasu[] w miejscu kolumny):
					filt = (df['email'] == 'md@gmail.com') 
					df.loc[filt,'last']=df.loc[filt,'last'].replace({'Dziak':'Czak'})
														 # 0    Czak
														 # Name: last, dtype: object
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------

1.PODSTAWOWE KOMENDY:
	A) ODCZYT PLIKU:
		1)odczyt danych z pliku csv:
				import pandas as pd
				df = pd.read_csv('sciezka/do/plikucsv.csv')
		ZZ)odczyt danych z dodatkowymi funkcjami:
			*odczyt danych ze zmianą indexu na inną kolumnę:
				df = pd.read_csv('sciezka/plikucsv.csv', index_col='email')
				
	B)INDEXY:
		*w Pandasie index nie musi być unique(chociaż zazwyczaj jest). 
		*numer wiersza to nie index.Index można zmienić na inną kolumnę. 
		*po zmianie indexu 'iloc' będzie działał bo jest po nr. wierszy
				df.set_index('email') 			  		 # jednorazowa zmiana
				df.set_index('email',inplace=True)	 	 # zmiana na stałe
				df.set_index(pd.Index([True, False, False, True], inplace = True)
														 # zm. na stałe indexu dla 
														 # tab. o określ. l. wierszy 
				df.reset_index(inplace=True)	 		 # zmiana na domyślny index
	
	C) DANE i INFO o DANYCH:
		*rozmiar tabeli (atrybut shape)"
				df.shape
		*informacje o danych i typach danych:
				df.info()
		*wyświetlenie wszystkich kolumn (atrybut columns):
				df.columns
				list(df)								 #lista kolumn
		*domyślny slicing:
			**na obiekcie Series:
					ser = pd.Series(lst)
					ser[3:]										# od 3 wiersza	
			**na obiekcie DataFrame:
					df = pd.DateFrame(dct)
					df[3:][:4]									# od 3 kolumny
																# i od 4 wiersza
		*WYŚWIETLANIE WIERSZY (atrybuty iloc, loc) (z użyciem slicingu):
			X1)*po numerze wiersza i po numerze kolumny (numer wiersza/kolumny to 
			nie to samo co numer indexu) (atrybut iloc - integer loc)(numery od 0):
				a)pojedyńczy wiersz
					df.iloc[2]							 # first    Lukas
														 # last     Lipka
														 # email    ll@gmail.com
														 # Name: 2, dtype: object
				b)kilka wierszy (musi byc podwójny nawias):
					df.iloc[[0,2]] 			    		 #	 first	last	email
														 # 0 Marek	Dziak	md@..
														 # 2 Lukas	Lipka	ll@..
				c)kilka wierszy ale tylko wybrane kolumny (liczone od 0):
					df.iloc[[0,2],1]			 		 # 0 Dziak
														 # 2 Lipka
														 # Name: last, dtype: object
					df.iloc[[0,2],[0,2]]		 		 #	 first	email
														 # 0 Marek	md@gmail.com
														 # 2 Lukas	ll@gmail.com
				d)wszystkie wiersze ale wybrane kolumny:
					df.iloc[:,[2,0]] 			 		 #	   email		first
														 # 0 md@gmail.com	Marek
														 # 1 an@gmail.com	Anna
														 # 2 ll@gmail.com	Lukas
				e)SLICING - kilka wierszy (od do) - slicing bez nawiasów [] 
				  nie włączający  ostatniego wiersza ani nie włączający ostatniej kolumny - inaczej niż w 'loc':
					df.iloc[0:2,[0,2]]			 		 #	 first	email
														 # 0 Marek	md@gmail.com
														 # 1 Anna 	an@gmail.com
														 # 2 Lukas	ll@gmail.com
					df.iloc[0:3,0:3]				 	 #	 first  last   email
														 # 0 Marek  Dziak  md@gm...
														 # 1 Anna   Nacz   an@gm...
														 # 2 Lukas  Lipka  ll@gm...
		    X2)*po nazwie indexu i po nazwie kolumn (a nie po numerach):
					df.set_index(pd.Index=['a','b','c'], inplace=True)
				a)pojedyńczy wiersz
					df.loc["c"]					 		 # first    Lukas
														 # last     Lipka
														 # email    ll@gmail.com
														 # Name: c, dtype: object
				b)kilka wierszy (musi byc podwójny nawias):
					df.loc[["a","c"]] 			     	 #	 first	last	email
														 # a Marek	Dziak	md@..
														 # c Lukas	Lipka	ll@..
				c)kilka wierszy ale tylko wybrane kolumny (po nazwie kolumny 
				 można wpisać dowolną kolejność kolumn):
					df.loc[["a","c"],'last']		 	 # a Dziak
														 # c Lipka
														 # Name: last, dtype: object
					df.loc[["a","c"],['email','first']]	 #     email		first
														 # a md@gmail.com	Marek
														 # c ll@gmail.com	Lukas
				d)wszystkie wiersze ale wybrane kolumny:
					df.loc[:,['email','first']] 		 #	   email		first
														 # a md@gmail.com	Marek
														 # b an@gmail.com	Anna
														 # c ll@gmail.com	Lukas
				e)SLICING - kilka wierszy (od do) - slicing bez nawiasów [] 
				  i włączający ostatni wiersz i ostatnią kolumnę (inaczej niż w iloc[])
					df.loc["a":"c",['email','first']]	 # 	   email		first
														 # a md@gmail.com	Marek
														 # b an@gmail.com	Anna 
														 # c ll@gmail.com	Lukas
					df.loc["a":"c",'first':'email]		 #	 first  last   email
														 # a Marek  Dziak  md@gm..
														 # b Anna   Nacz   an@gm..
														 # c Lukas  Lipka  ll@gm..
		    X3)*odczyt wierszy po SLICINGu:
				a)dla domyślnego indexu zwróci przedział zamknięty z lewej strony 
				  i otwarty z prawej:
					df[2:3]								 # wyświetli tylko 2 wiersz
					df[2:5]								 # wyświetli wiersze 2,3,4
				  można użyć tez loc[]/iloc[] (przedziały są obustronnie zamknięte)
					df.loc[2]
					df.loc[2:4]
				c)przy zmianie indexu na kolumnę z datą można filtrować te daty jak 
				  index domyślny czyli po slicingu (od do), przy czym nie trzeba 
				  wpisywać np całej daty a wystarczy jej początek:
					df.set_index('Date', inplace = True)
					df['2020-01':'2020-02']	#wiersze z datami od Stycznia do Lutego 
					df['2020':'2020']		#wiersze tylko z roku 2020
				  można użyć tez loc[]/iloc[] (przedziały TEŻ są obustronnie zamk.)
					df.loc['2020-01':'2020-01']
					df.loc['2020':'2020']
				  w przypadku gdyby data nie była indexem to zakres dat uzyskujesz 
				  tak jak przy zwykłym filtrowaniu:
					filt = (df['Date']>='2020-01') & (df['Date']<='2020-02')
					df.loc[filt]
			X4)*po BOOLEAN MASK - tylko tych wierszy gdzie maska wskazuję True:
				dict = {'name':["mar", "ewe", "luk", "kar"],
						'degree': ["MBA", "BCA", "M.Tech", "MBA"]}
				df = pd.DataFrame(dict, index = [0, 1, 2, 3])
				print(df[[True, False, True, True]])			# 	name	degree
																# 0	mar		MBA
																# 2	luk		M.Tech
																# 3	kar		MBA
	D)FILTROWANIE:
		1*PODSTAWY:
			*nie wolno dać nazwy 'filter' do zmiennej 
					filt = (df['last'] == 'Dziak') 
					filt								 # 0     True
														 # 1    False
														 # 2    False
														 # Name: last, dtype: bool
			*zwrócenie danych (wszystkich wierszy gdzie jest True):
				**df[...] - tak jakbyś wywoływał KOLUMNĘ 
					df[filt] / df[(df['last'] =='Dziak')]
				**df.loc[...] - tak jakbyś wywoływął wiersze/komórkę
					df.loc[filt]
					df.loc[filt, 'first']
			*OPERATOR ZAPRZECZENIA: '~' 
					df.loc[~filt]
			*SPÓJNIKI LOGICZNE: '&', '|':
					filt1= ((df['last'] == 'Nacz') & (df['first'] == 'Anna'))
					filt2= ((df['last'] == 'Nacz') | (df['first'] == 'Marek'))
			*Narzędzie PORÓWNIANIA: '>','<','==','!=':
					above_average_salary = (df['Salary'] > 70000)
		2*FUNKCJE:
			*isin(list):
					countries = ['United States', 'India', 'United Kingdom']
					filt = df['Country'].isin(countries)
					df.loc[filt,'Lang'] 			  	# pokazuję tylko col. Lang
														# z przefiltrowanymi danymi
			*str.contains('string', na=False) - dla filtrowania stringów 
			 (trzeba dodać 2gi arg na=False żeby nie wyrzucało błędów)
					filt = df['Lang'].str.contains('Python', na=False)
					df.loc[filt,['Country','Lang']] 	# pokazuję tylko 2 kolumny
														# z przefiltrowanymi danymi
					df.loc[filt, 'LanguageWorkedWith']
	E)SORTOWANIE i LIMIT SOTUJĄCY
		1*Sortowanie po indexie:
					df.sort_index(inplace=True)					 #rosnące na stałe
					df.sort_index(ascending=False, inplace=True) #malejące na stałe
		2*Sortowanie po wartościach:
			*po 1 kolumnie (rosnąco i malejąco i na stałe):
					df.sort_values(by='last')		
					df.sort_values(by='last', ascending=False)	
					df.sort_values(by='last', ascending=False, inplace=True)
					df.sort_values(by='last', ascending=False, inplace=True, 
				*argument: na_position = 'first' / 'last'- wartości np.nan/None w 
				 kolumnie 'last' będą widoczne jako pierwsze lub jako ostatnie:
					df.sort_values(by='last', ascending=False, inplace=True, 
															na_position='last')
			*po wielu kolumnach - liczba elem. w liście dla 'ascending' musi 
			 odpowidać l. elem. w liście dla 'by':
					df.sort_values(by=['last', 'first'], ascending=False, 	
																inplace=True)
					df.sort_values(by=['last', 'first'],
										ascending=[False,True], inplace=True)
		3*LIMIT SORTUJĄCY - największe/najmniejsze wartosci z limitem np.10:
			*na pojedyńczej kolumnie:
					df['ConvertedComp'].nlargest(10)
			*na całej tabeli wzgledem danej kolumny 
					df.nsmallest(10,'ConvertedComp')
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
**dobre tutoriale pandasa na 'data school'		
0* Why Pandas? ------> Pandas are used in conjunction with other libraries that are used for data science (matplotlib - plotting functions; numpy - functions, scipy - statistical functions; scikit-learn (machine learning alghoritms)
1.DATA OBJECTS in PANDAS:
	A)Dane jako obiekt DataFrame (słownik z kluczami jako kolumny i wartościami jako 
	  komórki) (taki słownik można zamienić na DataFrame i można dostać się do jego 
	  poszczególnych komórek):
				people = {
					"first": ['Marek', 'Anna', 'Lukas'],
					"last": ['Dziak', 'Nacz', 'Lipka'],
					"email": ['md@gmail.com','an@gmail.com','ll@gmail.com']
				}
				df = pd.DataFrame(people)
				df
		*Series - 1wymiarowa tabela - zwrócenie poszczególnej kolumny (lepiej używać 
		 formy z nawiasami bo jest możliwośc że nazwa kolumny może miec taką samą 
		 nazwę jak atrybut/metoda DataFrame'u):
				df['last']		/				 	# 0	Dziak
				df.last							 	# 1	Nacz
													# 2	Lipka
				type(df['last']) / type(df.last) # pandas.core.series.Series
		*DataFrame - 2wymiarowa tabela - zwrócenie kilku kolumn (musi być 
		 podwójny nawias bo inaczej KeyError):
				df[['first', 'last']]		 		# 0 Marek	Dziak
													# 1	Anna	Nacz
													# 2	Lukas	Lipka
				type(df1[['first', 'last']])  		# pandas.core.frame.DataFrame
	B)Wyświetlenie danych:
		*wszystkich:
				df
		*bezwzględnie wszystkich (w liczbie kol/wierszy wpisujesz max liczbę) 
				pd.set_option('display.max_columns', 85)
				pd.set_option('display.max_rows', 3000)
				df
		*pierwszych/ostatnich 5ciu/podanej liczby wierszy:
				df.head()
				df.tail()
				df.head(12)
				df.tail(12)
		*poszczególnej kolumny / kilku kolumn:
				df['last']		
				df[['last', 'email']]					 #musi byc podwójny nawias	

	C)Różne opcje tworzenia Series i DataFrame:			
		*SERIES:
			**puste Series:
					ser = pd.Series()							# Series([], dtype: 
																# float64)
			**Series z 1 wartości:
					ser = pd.Series(0)							# 0    0
					print(ser)									# dtype: int64
			**Series z listy:
					lst=['a','b']								# 0    a
					ser = pd.Series(lst)						# 1    b
					print(ser)									# dtype: object
			**Series z tablic numpy'owych:
					import numpy as np							# 0    g
					data= np.array(['g','e','e'])				# 1    e
					ser = pd.Series(data)						# 2    e
																# dtype: object	
		      *z przypisaniem indexów do wartości
					data= np.array(['g','e'], index=[10,11])	# 10   g
					ser = pd.Series(data)						# 11   e
																# dtype: object	
			**Series ze słownika:
					dct = {'a':[22,24], 'b':[33,35]}			# a  [22,24]
					ser = pd.Series(dct)						# b  [33,35]
																# dtype: int64
			***FUNKCJE na SERIES (z przykładami)(na samym końcu strony) WRAZ Z BINARY
			OPERATIONS (operacje binarne na kilku kolumnach (np dodawania tych samych
			indeksów w danej kolumnie związanej z obiektem Series)
			https://www.geeksforgeeks.org/python-pandas-series/
		*DATAFRAME:
			**pusty DataFrame:
					df = pd.DataFrame()							# Empty DataFrame
					print(df)									# Columns: []
																# Index: []
			**DataFrame z listy:
					lst = ['g','e','e']							#	 0
					df = pd.DataFrame(lst)						# 0  g
					print(df)									# 1  e
																# 2  e
					lstOfLsts = [[1,2,3],[5,6,7],[7,8,9]], 
					df = pd.DateFrame(lstOfLsts, columns = ['a','b','c']
					print(df)									#    a   b   c 
																# 0  1   2   3
																# 1  4   5   6
																# 2  7   8   9
			**DataFrame z tablic numpy'owych:
					import numpy as np							#    YY   ZZ
					dicta = {'YY':np.array(["a", "b", "c"]),	# 0  a    10
							'ZZ':np.array([10, 20, 30])}		# 1  b    20
					df = pd.DataFrame(dicta)					# 2  c    30
			**DataFrame ze słownika:
					data = {'Name':['t', 'n', 'k'],				# 	Name   Age
							'Age':[20, 21, 19]}					# 0  t     20
					df = pd.DataFrame(data)						# 1  n     21
																# 2  k	   19
			***FUNKCJE na DATAFRAME'ach z przykładami (na samym końcu strony):
			https://www.geeksforgeeks.org/python-pandas-dataframe/?ref=lbp
	D)ITEROWANIE po danych:
					dict = {'YY':["a", "b", "c"],
							'ZZ':[10, 20, 30]}
					df = pd.DataFrame(dict)
		*PO KOLUMNIE - iteritems() - zwraca obiekt generatora składający się z nazwy  
		 kolumny oraz obiektu Series zwracającego wartości kolumny
					for i,j in df.iteritems():	
						print(i, type(i))	# YY <class 'str'>
						print(j, type(j))	# 0    a
											# 1    b
											# 2    c
											# Name: AAA, dtype: object <class 'pandas.core.series.Series'>
											# ZZ <class 'str'>
											# 0    10
											# 1    20
											# 2    30
											# Name: BBB, dtype: object <class 'pandas.core.series.Series'>
		*PO WIERSZU - iterrows() - zwraca obiekt generatora składający się z numeru
 		 wiersza oraz obiektu Series zwracającego nazwy kolumn i wartości dla wiersza
					for i, j in df.iterrows():
						print(i, type(i))	# 0 <class 'int'>
						print(j, type(j))	# YY     a
											# ZZ    10
											# Name: 0, dtype: object <class 'pandas.core.series.Series'>
											# 1 <class 'int'>
											# YY     b
											# ZZ    20
											# Name: 1, dtype: object <class 'pandas.core.series.Series'>
											# 2 <class 'int'>
											# YY     c
											# ZZ    30
											# Name: 2, dtype: object <class 'pandas.core.series.Series'>
		*PO CAŁYCH WIERSZACH - itertuples() - zwraca obiekt map składający się z 
		 obiektów <class 'pandas.core.frame.Pandas'>
					for i in df.itertuples():
						print(i)			# Pandas(Index=0, YY='a', ZZ=10) 
						print(type(i))		# <class 'pandas.core.frame.Pandas'>
											# Pandas(Index=1, YY='b', ZZ=20) 
											# <class 'pandas.core.frame.Pandas'>
											# Pandas(Index=2, YY='c', ZZ=30) 
											# <class 'pandas.core.frame.Pandas'>
											# Pandas(Index=3, YY='d', ZZ=40) 
											# <class 'pandas.core.frame.Pandas'>
0.INSTALACJA i otwarcie JUPYTER LAB + (anaconda):
	A)INSTALACJA w terminalu:
				pip install pandas 
				pip install notebook
				pip install jupyterlab
	B)JUPYTER NOTEBOOK:
		a)W TERMINALU będąc w folderze z danymi otwarcie poprzez:
				jupyter notebook
		  i automatyczne przejście na stronę 
				http://localhost:8888/notebooks/...
		  po zakończeniu pracy plik z rozszerzeniem .ipynb zostanie zapisany
		b)SKRÓTY JUPYTER NOTEBOOKa:
			*RUN komórki ---> SHIFT + ENTER
			*Restart and Run ----> Kernell -> Restart and Run all
			*TAB --> podpowiedź możliwych funkcji do użycia
			*SHIFT + TAB  --> podpowiedź funkcji wraz z dokuentacją
		c)ANACONDA: https://www.geeksforgeeks.org/how-to-install-python-pandas-on-windows-and-linux/
		
		
		
		
		